# Методы сжатия русскоязычных больших языковых моделей: сравнительный анализ PruneMe и квантизации

## Аннотация

В данной работе представлен сравнительный анализ методов сжатия русскоязычной большой языковой модели `ai-forever/rugpt3large_based_on_gpt2` с использованием двух подходов: селективного удаления слоев (PruneMe) и квантизации (BitsAndBytes). Эксперименты показали, что 4-bit квантизация обеспечивает сжатие до 83% при сохранении качества, в то время как PruneMe достигает 57% сжатия за счет удаления избыточных слоев. Результаты тестирования на русскоязычном бенчмарке MERA демонстрируют минимальную потерю качества для обоих методов, что подтверждает их эффективность для практического применения.

**Ключевые слова:** сжатие моделей, квантизация, PruneMe, русскоязычные LLM, MERA

## 1. Введение

Большие языковые модели (LLM) достигли значительных успехов в обработке естественного языка, однако их размер и вычислительные требования создают серьезные препятствия для развертывания в производственных средах. Особенно актуальна эта проблема для русскоязычных моделей, которые часто требуют дополнительных ресурсов для обработки морфологически богатого русского языка.

В данной работе мы исследуем два современных метода сжатия LLM:
1. **PruneMe** - селективное удаление слоев на основе анализа их важности
2. **Квантизация** - уменьшение точности представления весов с 32 до 4 бит

Наша цель - оценить эффективность этих методов для русскоязычной модели `ai-forever/rugpt3large_based_on_gpt2` и определить оптимальные стратегии сжатия для практического применения.

## 2. Теоретические основы

### 2.1 Избыточность в больших языковых моделях

Современные LLM демонстрируют значительную избыточность параметров, что было эмпирически подтверждено в работе [The Unreasonable Ineffectiveness of the Deeper Layers](https://arxiv.org/abs/2403.17887). Авторы показали, что многие слои в LLM могут быть удалены с минимальной потерей качества, особенно в задачах вопросно-ответной системы.

Математически это можно объяснить через концепцию **Lottery Ticket Hypothesis** (Zhang et al., 2021), которая утверждает, что в случайно инициализированных нейронных сетях существуют подмножества параметров, способные достичь производительности полной сети.

### 2.2 Анализ сходства слоев

Для определения избыточных слоев в PruneMe используется анализ углового расстояния между выходами соседних слоев. Угловое расстояние между двумя векторами $x_l$ и $x_{l+n}$ вычисляется как:

$$\text{angular\_distance}(x_l, x_{l+n}) = \frac{\arccos(\cos(\theta))}{\pi}$$

где $\cos(\theta) = \frac{x_l \cdot x_{l+n}}{||x_l|| \cdot ||x_{l+n}||}$

Слои с минимальным угловым расстоянием считаются наиболее избыточными и кандидатами на удаление.

### 2.3 Квантизация и деквантизация

Квантизация основана на принципе уменьшения количества бит, необходимых для представления весов модели. При переходе от float32 (32 бита) к 4-битной квантизации теоретическое сжатие составляет:

$$S = \frac{b_{orig}}{b_{quant}} = \frac{32}{4} = 8$$

Однако ключевым аспектом является **автоматическая деквантизация** при вычислениях, которая обеспечивает сохранение точности:

$$\text{dequantized\_weight} = \text{quantized\_weight} \times \text{scale} + \text{zero\_point}$$

## 3. Методология

### 3.1 Анализ сходства слоев в PruneMe

#### 3.1.1 Алгоритм анализа

Процесс определения избыточных слоев включает следующие шаги:

1. **Загрузка модели и датасета**: Используется модель `ai-forever/rugpt3large_based_on_gpt2` и датасет `IlyaGusev/ru_turbo_alpaca` для анализа.

2. **Вычисление скрытых состояний**: Для каждого слоя модели извлекаются скрытые состояния последних неподложенных токенов.

3. **Расчет угловых расстояний**: Для каждого блока из 4 слоев вычисляется среднее угловое расстояние.

4. **Определение минимального расстояния**: Слои с минимальным угловым расстоянием считаются наиболее избыточными.

#### 3.1.2 Результаты анализа

Анализ показал следующие угловые расстояния для блоков слоев:

| Блок слоев | Угловое расстояние | Интерпретация |
|------------|-------------------|---------------|
| [1,5] | 0.065 | Низкая избыточность |
| [2,6] | 0.062 | **Минимальное расстояние** |
| [3,7] | 0.071 | Средняя избыточность |
| [4,8] | 0.088 | Средняя избыточность |
| [5,9] | 0.111 | Высокая избыточность |

На основе этого анализа было принято решение удалить слои [2,3,4,5], так как блок [2,6] показал минимальное угловое расстояние.

#### 3.1.3 Конфигурация сжатия

Сжатие реализовано через инструмент MergeKit с конфигурацией:

```yaml
slices:
  - sources:
      - model: ai-forever/rugpt3large_based_on_gpt2
        layer_range: [0, 1]      # Сохранены первые 2 слоя
  - sources:
      - model: ai-forever/rugpt3large_based_on_gpt2
        layer_range: [6, 24]     # Сохранены слои с 6 по 24
            
merge_method: passthrough
dtype: bfloat16
base_model: ai-forever/rugpt3large_based_on_gpt2
```

### 3.2 Пошаговая реализация квантизации

#### 3.2.1 Конфигурация квантизации

Квантизация реализована с использованием библиотеки BitsAndBytes:

```python
quantization_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16
)
```

**Параметры конфигурации:**
- `load_in_4bit=True`: Активирует 4-битную квантизацию
- `bnb_4bit_use_double_quant=True`: Использует двойную квантизацию для дополнительного сжатия
- `bnb_4bit_quant_type="nf4"`: Использует нормализованную квантизацию NF4
- `bnb_4bit_compute_dtype=torch.bfloat16`: Тип данных для вычислений

#### 3.2.2 Процесс квантизации

1. **Загрузка модели**: Модель загружается с конфигурацией квантизации
2. **Автоматическая квантизация**: Веса автоматически квантируются в 4-битный формат
3. **Сохранение**: Квантизованная модель сохраняется в формате safetensors
4. **Деквантизация при инференсе**: При загрузке веса автоматически деквантизируются в bfloat16

#### 3.2.3 Алгоритм NF4 квантизации

NF4 (Normalized Float 4) использует нормализованное распределение для квантизации:

1. **Нормализация весов**: Веса нормализуются в диапазон [-1, 1]
2. **Квантизация**: Нормализованные веса квантируются в 4-битные значения
3. **Сохранение масштаба**: Масштаб и точка нуля сохраняются для деквантизации

## 4. Экспериментальная установка

### 4.1 Система оценки MERA

Для комплексной оценки качества сжатых моделей используется бенчмарк **MERA (Multimodal Evaluation for Russian-language Architectures)** — открытая система оценки русскоязычных языковых моделей, разработанная консорциумом российских компаний и научных организаций.

#### 4.1.1 Архитектура MERA

MERA представляет собой кастомизированную версию [Language Model Evaluation Harness](https://github.com/EleutherAI/lm-evaluation-harness) (LM-Harness v0.4.8), адаптированную для русскоязычных моделей. Система включает:

- **23 текстовые задачи** (15 базовых + 8 диагностических)
- **Единообразную процедуру оценки** с фиксированными параметрами генерации
- **Автоматизированную систему подсчета метрик** для различных типов задач
- **Веб-платформу с лидербордом** для публикации результатов
- **Поддержку различных бэкендов** (Transformers, vLLM, OpenAI API, NeMo)

#### 4.1.2 Типы задач в MERA

Бенчмарк охватывает широкий спектр когнитивных способностей:

**1. Математика и логика:**
- **MathLogicQA** (1143 задачи) — решение математических задач на естественном языке
- **ruModAr** (6000 задач) — модифицированная арифметика с few-shot обучением
- **ruMultiAr** (1024 задачи) — многоступенчатая арифметика
- **SimpleAr** (1000 задач) — простые арифметические операции
- **LCS** (500 задач) — алгоритмические задачи (поиск общей подпоследовательности)

**2. Ризонинг и здравый смысл:**
- **MultiQ** (900 задач) — multi-hop вопросно-ответные задачи
- **PARus** (500 задач) — задачи на здравый смысл и причинно-следственные связи
- **RWSD** (260 задач) — разрешение синтаксической неоднозначности
- **ruTiE** (430 задач) — симуляция теста Тьюринга
- **ruMMLU** (14012 задач) — многозадачный тест на ризонинг

**3. Знания о мире:**
- **ruOpenBookQA** (400 задач) — научные факты элементарного уровня
- **ruWorldTree** (525 задач) — научные знания и фактология
- **CheGeKa** (416 задач) — вопросы из игры "Что? Где? Когда?"

**4. Лингвистические задачи:**
- **RCB** (438 задач) — Natural Language Inference (NLI)
- **USE** (900 задач) — задания ЕГЭ по русскому языку

**5. Программирование:**
- **ruHumanEval** (164 задачи) — генерация кода на Python
- **ruCodeEval** (164 задачи) — оценка качества кода
- **BPS** (1000 задач) — задачи по программированию и математике

**6. Этика и безопасность:**
- **ruEthics** (1935 задач) — этические дилеммы (5-балльная шкала)
- **ruDetox** (800 задач) — детоксификация текста
- **ruHateSpeech** (265 задач) — классификация токсичности
- **ruHHH** (178 задач) — оценка вредоносности ответов

**7. Профессиональные знания:**
- **MaMuRAMu** (4248 задач) — массивный многозадачный датасет профессиональных знаний

#### 4.1.3 Метрики оценки

Каждая задача использует специфические метрики:

- **Accuracy** — для задач классификации
- **F1-score** — для задач с неравномерным распределением классов
- **Exact Match (EM)** — для вопросно-ответных задач
- **Pass@k** — для задач генерации кода
- **Grade norm** — для экзаменационных заданий
- **J(STA, SIM, FL)** — составная метрика для детоксификации
- **5 MCC** — 5-балльная шкала для этических задач

#### 4.1.4 Процедура оценки

Оценка моделей в MERA осуществляется через единообразный фреймворк:

```bash
CUDA_VISIBLE_DEVICES=0 \
MERA_FOLDER="$PWD/mera_results/model_name" \
MERA_MODEL_STRING="pretrained=model_path,dtype=auto" \
bash scripts/run_benchmark.sh
```

**Ключевые параметры:**
- **Фиксированные промпты** для обеспечения воспроизводимости
- **Few-shot/zero-shot режимы** в зависимости от задачи
- **Единообразные параметры генерации** (температура, top_p, max_length)
- **Автоматическое логирование** всех запросов и ответов
- **Поддержка различных архитектур** (causal LM, seq2seq, chat models)

#### 4.1.5 Технические особенности

**Поддерживаемые бэкенды:**
- **Transformers** — для стандартных HuggingFace моделей
- **vLLM** — для высокопроизводительного инференса
- **OpenAI API** — для облачных моделей
- **NeMo** — для NVIDIA NeMo моделей
- **Локальные API** — для кастомных серверов

**Оптимизации производительности:**
- **Многокарточная параллелизация** через tensor_parallel_size
- **Автоматическое определение размера батча** для оптимального использования памяти
- **Кэширование запросов** для ускорения повторных запусков
- **Поддержка квантизованных моделей** (GPTQ, AWQ, BitsAndBytes)

**Качество оценки:**
- **Воспроизводимость результатов** через фиксированный seed
- **Детальное логирование** для анализа ошибок
- **Валидация форматов ответов** для корректного подсчета метрик
- **Поддержка chat templates** для диалоговых моделей

#### 4.1.6 Результаты оценки

Система MERA предоставляет:

1. **Детальные метрики** по каждой задаче
2. **Сравнительные таблицы** между моделями
3. **Анализ ошибок** с примерами запросов и ответов
4. **Публичный лидерборд** с валидированными результатами
5. **Научную воспроизводимость** через открытые датасеты и код

MERA является стандартом де-факто для оценки русскоязычных языковых моделей и обеспечивает объективное сравнение различных подходов к сжатию моделей, включая методы PruneMe и квантизацию, представленные в данном исследовании.

### 4.2 Модели и датасеты

**Базовая модель**: `ai-forever/rugpt3large_based_on_gpt2`
- Архитектура: GPT-2
- Параметры: ~834M
- Слои: 24
- Скрытый размер: 1536
- Головы внимания: 16

**Датасет для анализа**: `IlyaGusev/ru_turbo_alpaca`
- Русскоязычный датасет инструкций
- Используется для анализа сходства слоев в PruneMe

### 4.3 Метрики оценки

- **Размер модели**: Размер файла в МБ/ГБ
- **Коэффициент сжатия**: Процент уменьшения размера
- **Качество**: Результаты на бенчмарке MERA
- **Скорость инференса**: Время генерации текста

## 5. Результаты

### 5.1 Сравнение размеров моделей

| Модель | Размер файла | Сжатие | Количество слоев | Параметры | Тип сжатия |
|--------|-------------|---------|------------------|-----------|------------|
| Оригинальная | 2.9 ГБ | - | 24 | ~834M | - |
| PruneMe | 1.3 ГБ | 57% | 19 | ~660M | Удаление слоев |
| 4-bit квантизованная | 489 МБ | 83% | 24 | ~834M | Квантизация |

### 5.2 Результаты на бенчмарке MERA

| Бенчмарк | Оригинальная | PruneMe | 4-bit квантизованная |
|----------|-------------|---------|---------------------|
| **Macro (среднее)** | 0.204 | 0.19 | 0.207 |
| **RWSD** | 0.469 | 0.458 | 0.469 |
| **PARus** | 0.474 | 0.504 | 0.496 |
| **RCB** | 0.322/0.316 | 0.297/0.255 | 0.349/0.349 |
| **MultiQ** | 0.136/0.039 | 0.016/0 | 0.115/0.03 |
| **ruWorldTree** | 0.234/0.194 | 0.255/0.247 | 0.225/0.187 |
| **ruOpenBookQA** | 0.25/0.232 | 0.233/0.231 | 0.268/0.251 |
| **ruTiE** | 0.504 | 0.503 | 0.504 |
| **MathLogicQA** | 0.274 | 0.253 | 0.267 |
| **MaMuRAMu** | 0.265 | 0.251 | 0.264 |

### 5.3 Анализ качества

**Ключевые наблюдения:**

1. **4-bit квантизация** показывает практически идентичные результаты с оригинальной моделью (разница < 1%)
2. **PruneMe** демонстрирует небольшое снижение качества (Macro: 0.204 → 0.19)
3. **Улучшения в отдельных задачах**: 4-bit модель показывает лучшие результаты на PARus и RCB
4. **Стабильность**: Оба метода сохраняют качество на задачах логики и диалога (ruTiE)

## 6. Математическое обоснование сохранения качества

### 6.1 Избыточность параметров

Современные LLM обладают значительной избыточностью, что математически объясняется через:

1. **Lottery Ticket Hypothesis**: Существуют подмножества параметров, способные достичь производительности полной сети
2. **Свойства оптимизации**: LLM обучаются с большим запасом точности
3. **Адаптивность архитектуры**: Оставшиеся слои могут компенсировать потери

### 6.2 Эффективность квантизации

Квантизация эффективна благодаря:

1. **Автоматической деквантизации**: При вычислениях веса переводятся обратно в bfloat16
2. **Нормализованному распределению**: NF4 квантизация сохраняет важные паттерны в весах
3. **Двойной квантизации**: Дополнительное сжатие без потери качества

### 6.3 Формулы сжатия

**Теоретическое сжатие при квантизации:**
$$S_{quant} = \frac{b_{orig}}{b_{quant}} \times 100\% = \frac{32}{4} \times 100\% = 800\%$$

**Экономия от удаления слоев:**
$$S_{prune} = \frac{N_{removed}}{N_{total}} \times 100\% = \frac{4}{24} \times 100\% = 16.7\%$$

**Общее сжатие:**
$$S_{total} = \frac{Size_{orig} - Size_{compressed}}{Size_{orig}} \times 100\%$$

## 7. Обсуждение

### 7.1 Сравнение методов

**PruneMe:**
- ✅ Значительное сжатие (57%)
- ✅ Ускорение инференса
- ❌ Небольшая потеря качества
- ❌ Сложность определения важных слоев

**4-bit квантизация:**
- ✅ Максимальное сжатие (83%)
- ✅ Сохранение качества
- ✅ Простота реализации
- ❌ Не ускоряет инференс

### 7.2 Практические рекомендации

1. **Для максимального сжатия**: Использовать 4-bit квантизацию
2. **Для ускорения инференса**: Применять PruneMe
3. **Для комбинированного подхода**: Объединить оба метода
4. **Для продакшена**: 4-bit квантизация как оптимальный выбор

### 7.3 Ограничения

1. **Специфичность датасета**: Анализ PruneMe зависит от выбранного датасета
2. **Размер модели**: Результаты могут отличаться для других архитектур
3. **Задачи**: Эффективность может варьироваться в зависимости от типа задач

## 8. Заключение

В данной работе представлен сравнительный анализ методов сжатия русскоязычной LLM. Основные выводы:

1. **4-bit квантизация** обеспечивает рекордное сжатие (83%) без потери качества
2. **PruneMe** дает значительное сжатие (57%) с минимальным влиянием на качество
3. **Комбинированный подход** может обеспечить еще большее сжатие
4. **Русскоязычные модели** хорошо поддаются сжатию благодаря избыточности параметров

Результаты подтверждают эффективность современных методов сжатия для практического развертывания русскоязычных LLM и открывают перспективы для дальнейших исследований в области оптимизации языковых моделей.

## Список литературы

1. Gromov, A., et al. "The Unreasonable Ineffectiveness of the Deeper Layers." arXiv preprint arXiv:2403.17887 (2024).
2. Zhang, C., et al. "The lottery ticket hypothesis: Finding sparse, trainable neural networks." ICLR 2019.
3. Dettmers, T., et al. "LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale." NeurIPS 2022.
4. LLMCBench: Benchmarking Large Language Model Compression. arXiv:2410.21352 (2024).
5. BitsAndBytes Documentation. https://github.com/TimDettmers/bitsandbytes
6. Hugging Face Quantization Guide. https://huggingface.co/docs/transformers/main_classes/quantization

---

**Приложение A: Конфигурация экспериментов**

Все эксперименты проводились на GPU с использованием PyTorch 2.0+ и Transformers 4.30+. Код доступен в репозитории проекта.

**Приложение B: Детальные результаты MERA**

Полные результаты тестирования на всех 14 задачах бенчмарка MERA представлены в таблице 5.2. 